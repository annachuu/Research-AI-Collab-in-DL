const HCIData=  [

    {
        "pnx": {
            "links": {
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "This paper presents a comprehensive exploration of the synergistic relationship between User-Centered Design (UCD) and Artificial Intelligence (AI) within the context of the AI-UCD Algorithm Framework. With the growing influence of AI in digital interfaces, the need to prioritize user needs and preferences has become paramount. The AI-UCD Framework, consisting of nine pivotal steps, acts as a structured guide for integrating AI into user interfaces while ensuring a user-centric, data-driven, and ethical approach. The exploration begins by highlighting the importance of understanding user needs and context through robust user research and contextual inquiry. It then delves into the process of defining AI integration objectives and brainstorming AI-enhanced solutions, emphasizing the creative aspects of UCD in tandem with AI capabilities. Subsequently, the paper discusses the critical role of designing AI-driven interfaces, from information architecture to user flow design, ensuring seamless integration of AI features.Implementation and testing of AI features are addressed, highlighting the collaboration between UI/UX designers and AI developers. The paper emphasizes the iterative nature of the framework, relying on usability testing and user feedback to drive continuous improvements. Moreover, it considers user training and assistance, a vital aspect of introducing users to AI features.The framework's data-driven aspect is covered by discussing data collection, analysis, and performance monitoring to ensure AI features are meeting objectives and KPIs. Additionally, the exploration addresses AI's role in personalization, adapting to user behavior and preferences. It recognizes the ethical dimensions of AI, promoting transparency, fairness, and accessibility.The paper then presents a five-step AI-UCD Validation Model, designed to verify the framework's effectiveness in real-world applications. These validation steps encompass user testing and feedback, data analysis, ethical audits, iterative improvements, and compliance with industry standards. Examples of how these steps work in practice are provided."
                ],
                "creator": [
                    "Siricharoien, Waralak Vongdoiwang"
                ],
                "title": [
                    "Elevating User-Centered Design with AI: A Comprehensive Exploration using the AI-UCD Algorithm Framework",
                    "EAI endorsed transactions on context-aware systems and applications."
                ],
                "issn": [
                    "2409-0026",
                    "2409-0026"
                ],
                "fulltext": [
                    "true"
                ],
                "startdate": [
                    "20240315"
                ],
                "enddate": [
                    "20240315"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNpNkMtOwzAQRS0EElXpjg_wB5AytuM82EVpCpUqsWnXkeOMk0BeskMLf08qumA1V5p77uIQ8shg7TOInhEnrZxa-5yxG7LgPsQeAA9u_-V7snLuAwBYKIXPYUHqrMWTmpq-okeH1kuxn9BiSTfomqqn52aqabJ7oQlNh260WGPvmhPS7HtsBzuTQ0-_3IWfapyb3jHd0KStBjuTHd1a1eF5sJ8P5M6o1uHqepfksM0O6Zu3f3_dpcne06FknhGyEIaxMBDSN5pxFUoZ6WJ-Qqh1LJgGBXFcAg9jjSqSRgI3ZQFBBIL5Ykme_ma1HZyzaPLRNp2yPzmD_OIpv3rKL57EL7C3W_c"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Siricharoien, Waralak Vongdoiwang"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "This paper presents a comprehensive exploration of the synergistic relationship between User-Centered Design (UCD) and Artificial Intelligence (AI) within the context of the AI-UCD Algorithm Framework. With the growing influence of AI in digital interfaces, the need to prioritize user needs and preferences has become paramount. The AI-UCD Framework, consisting of nine pivotal steps, acts as a structured guide for integrating AI into user interfaces while ensuring a user-centric, data-driven, and ethical approach. The exploration begins by highlighting the importance of understanding user needs and context through robust user research and contextual inquiry. It then delves into the process of defining AI integration objectives and brainstorming AI-enhanced solutions, emphasizing the creative aspects of UCD in tandem with AI capabilities. Subsequently, the paper discusses the critical role of designing AI-driven interfaces, from information architecture to user flow design, ensuring seamless integration of AI features.Implementation and testing of AI features are addressed, highlighting the collaboration between UI/UX designers and AI developers. The paper emphasizes the iterative nature of the framework, relying on usability testing and user feedback to drive continuous improvements. Moreover, it considers user training and assistance, a vital aspect of introducing users to AI features.The framework's data-driven aspect is covered by discussing data collection, analysis, and performance monitoring to ensure AI features are meeting objectives and KPIs. Additionally, the exploration addresses AI's role in personalization, adapting to user behavior and preferences. It recognizes the ethical dimensions of AI, promoting transparency, fairness, and accessibility.The paper then presents a five-step AI-UCD Validation Model, designed to verify the framework's effectiveness in real-world applications. These validation steps encompass user testing and feedback, data analysis, ethical audits, iterative improvements, and compliance with industry standards. Examples of how these steps work in practice are provided."
                ],
                "ispartof": [
                    "EAI endorsed transactions on context-aware systems and applications., 2024-03, Vol.10"
                ],
                "identifier": [
                    "ISSN: 2409-0026",
                    "EISSN: 2409-0026",
                    "DOI: 10.4108/eetcasa.4211"
                ],
                "snippet": [
                    "This paper presents a comprehensive exploration of the synergistic relationship between User-Centered Design (UCD..."
                ],
                "creator": [
                    "Siricharoien, Waralak Vongdoiwang"
                ],
                "title": [
                    "Elevating User-Centered Design with AI: A Comprehensive Exploration using the AI-UCD Algorithm Framework"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "Freely available EZB journals"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "facets": {
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Siricharoien, Waralak Vongdoiwang"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-c751-f35b3f1176354fc12a7558cbc7507cc931c0a099d0279cea85f502fdb06803143"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "EAI endorsed transactions on context-aware systems and applications."
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "doi": [
                    "10.4108/eetcasa.4211"
                ],
                "au": [
                    "Siricharoien, Waralak Vongdoiwang"
                ],
                "atitle": [
                    "Elevating User-Centered Design with AI: A Comprehensive Exploration using the AI-UCD Algorithm Framework"
                ],
                "date": [
                    "2024-03-15"
                ],
                "risdate": [
                    "2024"
                ],
                "volume": [
                    "10"
                ],
                "eissn": [
                    "2409-0026"
                ],
                "issn": [
                    "2409-0026"
                ],
                "abstract": [
                    "This paper presents a comprehensive exploration of the synergistic relationship between User-Centered Design (UCD) and Artificial Intelligence (AI) within the context of the AI-UCD Algorithm Framework. With the growing influence of AI in digital interfaces, the need to prioritize user needs and preferences has become paramount. The AI-UCD Framework, consisting of nine pivotal steps, acts as a structured guide for integrating AI into user interfaces while ensuring a user-centric, data-driven, and ethical approach. The exploration begins by highlighting the importance of understanding user needs and context through robust user research and contextual inquiry. It then delves into the process of defining AI integration objectives and brainstorming AI-enhanced solutions, emphasizing the creative aspects of UCD in tandem with AI capabilities. Subsequently, the paper discusses the critical role of designing AI-driven interfaces, from information architecture to user flow design, ensuring seamless integration of AI features.Implementation and testing of AI features are addressed, highlighting the collaboration between UI/UX designers and AI developers. The paper emphasizes the iterative nature of the framework, relying on usability testing and user feedback to drive continuous improvements. Moreover, it considers user training and assistance, a vital aspect of introducing users to AI features.The framework's data-driven aspect is covered by discussing data collection, analysis, and performance monitoring to ensure AI features are meeting objectives and KPIs. Additionally, the exploration addresses AI's role in personalization, adapting to user behavior and preferences. It recognizes the ethical dimensions of AI, promoting transparency, fairness, and accessibility.The paper then presents a five-step AI-UCD Validation Model, designed to verify the framework's effectiveness in real-world applications. These validation steps encompass user testing and feedback, data analysis, ethical audits, iterative improvements, and compliance with industry standards. Examples of how these steps work in practice are provided."
                ],
                "jtitle": [
                    "EAI endorsed transactions on context-aware systems and applications."
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Elevating User-Centered Design with AI: A Comprehensive Exploration using the AI-UCD Algorithm Framework"
                ],
                "creationdate": [
                    "20240315"
                ],
                "author": [
                    "Siricharoien, Waralak Vongdoiwang"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "crossref"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_4108_eetcasa_4211"
                ],
                "addsrcrecordid": [
                    "eNpNkMtOwzAQRS0EElXpjg_wB5AytuM82EVpCpUqsWnXkeOMk0BeskMLf08qumA1V5p77uIQ8shg7TOInhEnrZxa-5yxG7LgPsQeAA9u_-V7snLuAwBYKIXPYUHqrMWTmpq-okeH1kuxn9BiSTfomqqn52aqabJ7oQlNh260WGPvmhPS7HtsBzuTQ0-_3IWfapyb3jHd0KStBjuTHd1a1eF5sJ8P5M6o1uHqepfksM0O6Zu3f3_dpcne06FknhGyEIaxMBDSN5pxFUoZ6WJ-Qqh1LJgGBXFcAg9jjSqSRgI3ZQFBBIL5Ykme_ma1HZyzaPLRNp2yPzmD_OIpv3rKL57EL7C3W_c"
                ],
                "sourcerecordid": [
                    "10_4108_eetcasa_4211"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-c751-f35b3f1176354fc12a7558cbc7507cc931c0a099d0279cea85f502fdb06803143"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.05572108"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:51:32&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-crossref&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=Elevating+User-Centered+Design+with+AI%3A+A+Comprehensive+Exploration+using+the+AI-UCD+Algorithm+Framework&rft.jtitle=EAI+endorsed+transactions+on+context-aware+systems+and+applications.&rft.au=Siricharoien%2C+Waralak+Vongdoiwang&rft.date=2024-03-15&rft.volume=10&rft.issn=2409-0026&rft.eissn=2409-0026&rft_id=info:doi/10.4108%2Feetcasa.4211&rft_dat=<crossref>10_4108_eetcasa_4211</crossref>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-c751-f35b3f1176354fc12a7558cbc7507cc931c0a099d0279cea85f502fdb06803143"
                ],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_4108_eetcasa_4211"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3593013.3594093$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0001-8784-289X",
                    "https://orcid.org/0009-0006-0241-3097",
                    "https://orcid.org/0000-0003-0557-3580",
                    "https://orcid.org/0000-0002-7161-7927"
                ],
                "creator": [
                    "Nigatu, Hellina Hailu",
                    "Pickoff-White, Lisa",
                    "Canny, John",
                    "Chasins, Sarah"
                ],
                "title": [
                    "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain",
                    "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
                ],
                "isbn": [
                    "9798400701924"
                ],
                "fulltext": [
                    "true"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20230612"
                ],
                "enddate": [
                    "20230612"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    ""
                ],
                "recordid": [
                    "eNqNkL1OwzAYRS0hJFDpzOqRJeXzT5yYDVJ-ValL9uiL7RRDYld2OsDTU0QfgOks597hEHLNYMWYLG9FqQUwsTpSghZnZKkrXUuACpjm8oIsc_4AAF4rzoW8JPsmFmuX_S74sKNDTLRNGPIekwvm645uXM4xZDqkONGHgx_tr4d0Hc1hcmGm27TD4L9x9jHQNsaR-kDnd0eb5CcfcKRvhzx7446TCX24IucDjtktT1yQ9umxbV6Kzfb5tbnfFMhVPReWVVIxicwqJXurZKWc1XaQqjLcWcG1ELKGwZU1lgbB9QagFLZmrOqdFgty83eLZur6GD9zx6D7bdSdGnWnRkd19U-165N3g_gB3w5odA"
                ],
                "scope": [
                    ""
                ],
                "creationdate": [
                    "2023"
                ],
                "creatorcontrib": [
                    "Nigatu, Hellina Hailu",
                    "Pickoff-White, Lisa",
                    "Canny, John",
                    "Chasins, Sarah"
                ],
                "rsrctype": [
                    "conference_proceeding"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice."
                ],
                "publisher": [
                    "New York, NY, USA: ACM"
                ],
                "ispartof": [
                    "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, 2023, p.1463-1478"
                ],
                "identifier": [
                    "ISBN: 9798400701924",
                    "DOI: 10.1145/3593013.3594093"
                ],
                "rights": [
                    "2023 Owner/Author"
                ],
                "snippet": [
                    ".... In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented..."
                ],
                "creator": [
                    "Nigatu, Hellina Hailu ; Pickoff-White, Lisa ; Canny, John ; Chasins, Sarah"
                ],
                "title": [
                    "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain"
                ],
                "type": [
                    "conference_proceeding"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "keyword": [
                    "Computing methodologies -- Artificial intelligence ;  Computing methodologies -- Machine learning ;  Human-centered computing -- Human computer interaction (HCI) -- HCI design and evaluation methods -- User studies ;  Human-centered computing -- Human computer interaction (HCI) -- Interaction paradigms ;  Human-centered computing -- Interaction design -- Interaction design process and methods -- Interface design prototyping ;  Human-centered computing -- Interaction design -- Interaction design process and methods -- User centered design ;  Human-centered computing -- Interaction design -- Interaction design process and methods -- User interface design"
                ]
            },
            "facets": {
                "creationdate": [
                    "2023"
                ],
                "creatorcontrib": [
                    "Nigatu, Hellina Hailu",
                    "Pickoff-White, Lisa",
                    "Canny, John",
                    "Chasins, Sarah"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a268t-d174614a1d664bd6476ed9df467c2ed32933480fe58a5ca0ebc0053d8117be93"
                ],
                "frbrtype": [
                    "5"
                ],
                "prefilter": [
                    "conference_proceedings"
                ],
                "rsrctype": [
                    "conference_proceedings"
                ]
            },
            "addata": {
                "format": [
                    "book"
                ],
                "ristype": [
                    "CONF"
                ],
                "cop": [
                    "New York, NY, USA"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3593013.3594093"
                ],
                "tpages": [
                    "16"
                ],
                "au": [
                    "Nigatu, Hellina Hailu",
                    "Pickoff-White, Lisa",
                    "Canny, John",
                    "Chasins, Sarah"
                ],
                "btitle": [
                    "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
                ],
                "atitle": [
                    "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain"
                ],
                "date": [
                    "2023-06-12"
                ],
                "risdate": [
                    "2023"
                ],
                "spage": [
                    "1463"
                ],
                "epage": [
                    "1478"
                ],
                "pages": [
                    "1463-1478"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0001-8784-289X",
                    "https://orcid.org/0009-0006-0241-3097",
                    "https://orcid.org/0000-0003-0557-3580",
                    "https://orcid.org/0000-0002-7161-7927"
                ],
                "isbn": [
                    "9798400701924"
                ],
                "abstract": [
                    "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice."
                ],
                "genre": [
                    "proceeding"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain"
                ],
                "creationdate": [
                    "20230612"
                ],
                "author": [
                    "Nigatu, Hellina Hailu ; Pickoff-White, Lisa ; Canny, John ; Chasins, Sarah"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    "acm"
                ],
                "recordid": [
                    "cdi_acm_books_10_1145_3593013_3594093"
                ],
                "addsrcrecordid": [
                    "eNqNkL1OwzAYRS0hJFDpzOqRJeXzT5yYDVJ-ValL9uiL7RRDYld2OsDTU0QfgOks597hEHLNYMWYLG9FqQUwsTpSghZnZKkrXUuACpjm8oIsc_4AAF4rzoW8JPsmFmuX_S74sKNDTLRNGPIekwvm645uXM4xZDqkONGHgx_tr4d0Hc1hcmGm27TD4L9x9jHQNsaR-kDnd0eb5CcfcKRvhzx7446TCX24IucDjtktT1yQ9umxbV6Kzfb5tbnfFMhVPReWVVIxicwqJXurZKWc1XaQqjLcWcG1ELKGwZU1lgbB9QagFLZmrOqdFgty83eLZur6GD9zx6D7bdSdGnWnRkd19U-165N3g_gB3w5odA"
                ],
                "sourcerecordid": [
                    "acm_books_10_1145_3593013_3594093"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a268t-d174614a1d664bd6476ed9df467c2ed32933480fe58a5ca0ebc0053d8117be93"
                ],
                "sourcetype": [
                    "Publisher"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.049821526"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:51:32&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm&rft_val_fmt=info:ofi/fmt:kev:mtx:book&rft.genre=proceeding&rft.atitle=Co-Designing+for+Transparency%3A+Lessons+from+Building+a+Document+Organization+Tool+in+the+Criminal+Justice+Domain&rft.btitle=Proceedings+of+the+2023+ACM+Conference+on+Fairness%2C+Accountability%2C+and+Transparency&rft.au=Nigatu%2C+Hellina+Hailu&rft.date=2023-06-12&rft.spage=1463&rft.epage=1478&rft.pages=1463-1478&rft.isbn=9798400701924&rft_id=info:doi/10.1145%2F3593013.3594093&rft.pub=ACM&rft.place=New+York%2C+NY%2C+USA&rft_dat=<acm>acm_books_10_1145_3593013_3594093</acm>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_acm_books_10_1145_3593013_3594093"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3613904.3642563$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Human-Centered AI prioritizes end-users’ needs like transparency and usability. This is vital for applications that affect people’s everyday lives, such as social assessment tasks in the public sector. This paper discusses our pioneering effort to involve public sector AI users in XAI application design through a co-creative workshop with unemployment consultants from Estonia. The workshop’s objectives were identifying user needs and creating novel XAI interfaces for the used AI system. As a result of our user-centered design approach, consultants were able to develop AI interface prototypes that would support them in creating success stories for their clients by getting detailed feedback and suggestions. We present a discussion on the value of co-creative design methods with end-users working in the public sector to improve AI application design and provide a summary of recommendations for practitioners and researchers working on AI systems in the public sector."
                ],
                "contributor": [
                    "Sas, Corina",
                    "Dugas, Phoebe Toups",
                    "Wilson, Max L.",
                    "Williamson, Julie R.",
                    "Shklovski, Irina",
                    "Kyburz, Penny",
                    "Mueller, Florian Floyd"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9975-2354",
                    "https://orcid.org/0009-0002-8616-0234",
                    "https://orcid.org/0000-0002-3516-6188",
                    "https://orcid.org/0000-0003-1001-2278",
                    "https://orcid.org/0000-0002-2367-162X"
                ],
                "creator": [
                    "Weitz, Katharina",
                    "Schlagowski, Ruben",
                    "André, Elisabeth",
                    "Männiste, Maris",
                    "George, Ceenu"
                ],
                "title": [
                    "Explaining It Your Way - Findings from a Co-Creative Design Workshop on Designing XAI Applications with AI End-Users from the Public Sector",
                    "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems"
                ],
                "isbn": [
                    "9798400703300"
                ],
                "fulltext": [
                    "true"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20240511"
                ],
                "enddate": [
                    "20240511"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    ""
                ],
                "recordid": [
                    "eNqNkM1Kw0AUhQdEUGrXbu_STepN5i-zLLHVQkFBS3UVMtOZdmibCZn49wy-tCnmAVxd-DjnwP0IuU5xkqaM31KRUoVsQgXLuKBnZKykyhmiREoRL8g4Rq-Rcy5ZlrFL8jP7ag6Vr329hUUHb-G9hXX1DQnMfb3paQTXhiNUUISkaG3V-Q8Ldzb6bQ3r0O7jLjQQ6gGdZl6nC5g2zcGbPhzqCJ--20EPZ_UmWUXbDpPdzsLTu-5z8GxNF9orcu6qQ7Tj4Y7Iaj57KR6S5eP9opgukyoTeZcIaTa5Q8dzm2kqHCpExTPlhJQ8Q8odTyk1SmuLpv9eS-0UdRRZbgwXho7I5G-3MsdSh7CPZYrlyV85-CsHf6VuvXV94eafBfoLPvRwqQ"
                ],
                "scope": [
                    ""
                ],
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Weitz, Katharina",
                    "Schlagowski, Ruben",
                    "André, Elisabeth",
                    "Männiste, Maris",
                    "George, Ceenu"
                ],
                "rsrctype": [
                    "conference_proceeding"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Human-Centered AI prioritizes end-users’ needs like transparency and usability. This is vital for applications that affect people’s everyday lives, such as social assessment tasks in the public sector. This paper discusses our pioneering effort to involve public sector AI users in XAI application design through a co-creative workshop with unemployment consultants from Estonia. The workshop’s objectives were identifying user needs and creating novel XAI interfaces for the used AI system. As a result of our user-centered design approach, consultants were able to develop AI interface prototypes that would support them in creating success stories for their clients by getting detailed feedback and suggestions. We present a discussion on the value of co-creative design methods with end-users working in the public sector to improve AI application design and provide a summary of recommendations for practitioners and researchers working on AI systems in the public sector."
                ],
                "publisher": [
                    "New York, NY, USA: ACM"
                ],
                "ispartof": [
                    "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 2024, p.1-14"
                ],
                "identifier": [
                    "ISBN: 9798400703300",
                    "DOI: 10.1145/3613904.3642563"
                ],
                "rights": [
                    "2024 Owner/Author"
                ],
                "snippet": [
                    ".... As a result of our user-centered design approach, consultants were able to develop AI interface prototypes that would support them in creating success stories for their clients by getting detailed..."
                ],
                "contributor": [
                    "Sas, Corina ; Dugas, Phoebe Toups ; Wilson, Max L. ; Williamson, Julie R. ; Shklovski, Irina ; Kyburz, Penny ; Mueller, Florian Floyd"
                ],
                "creator": [
                    "Weitz, Katharina ; Schlagowski, Ruben ; André, Elisabeth ; Männiste, Maris ; George, Ceenu"
                ],
                "title": [
                    "Explaining It Your Way - Findings from a Co-Creative Design Workshop on Designing XAI Applications with AI End-Users from the Public Sector"
                ],
                "type": [
                    "conference_proceeding"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "keyword": [
                    "Human-centered computing -- Human computer interaction (HCI) -- Empirical studies in HCI ;  Human-centered computing -- Human computer interaction (HCI) -- HCI design and evaluation methods -- User studies ;  Human-centered computing -- Interaction design -- Interaction design process and methods -- User centered design"
                ]
            },
            "facets": {
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Weitz, Katharina",
                    "Schlagowski, Ruben",
                    "André, Elisabeth",
                    "Männiste, Maris",
                    "George, Ceenu"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a268t-67cd8f0f58e2b36f09009529f67752035f5133c9bbe0c798b7bf93f3048cc56c3"
                ],
                "frbrtype": [
                    "5"
                ],
                "prefilter": [
                    "conference_proceedings"
                ],
                "rsrctype": [
                    "conference_proceedings"
                ]
            },
            "addata": {
                "format": [
                    "book"
                ],
                "ristype": [
                    "CONF"
                ],
                "cop": [
                    "New York, NY, USA"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3613904.3642563"
                ],
                "tpages": [
                    "14"
                ],
                "au": [
                    "Weitz, Katharina",
                    "Schlagowski, Ruben",
                    "André, Elisabeth",
                    "Männiste, Maris",
                    "George, Ceenu"
                ],
                "addau": [
                    "Sas, Corina",
                    "Dugas, Phoebe Toups",
                    "Wilson, Max L.",
                    "Williamson, Julie R.",
                    "Shklovski, Irina",
                    "Kyburz, Penny",
                    "Mueller, Florian Floyd"
                ],
                "btitle": [
                    "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems"
                ],
                "atitle": [
                    "Explaining It Your Way - Findings from a Co-Creative Design Workshop on Designing XAI Applications with AI End-Users from the Public Sector"
                ],
                "date": [
                    "2024-05-11"
                ],
                "risdate": [
                    "2024"
                ],
                "spage": [
                    "1"
                ],
                "epage": [
                    "14"
                ],
                "pages": [
                    "1-14"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9975-2354",
                    "https://orcid.org/0009-0002-8616-0234",
                    "https://orcid.org/0000-0002-3516-6188",
                    "https://orcid.org/0000-0003-1001-2278",
                    "https://orcid.org/0000-0002-2367-162X"
                ],
                "isbn": [
                    "9798400703300"
                ],
                "abstract": [
                    "Human-Centered AI prioritizes end-users’ needs like transparency and usability. This is vital for applications that affect people’s everyday lives, such as social assessment tasks in the public sector. This paper discusses our pioneering effort to involve public sector AI users in XAI application design through a co-creative workshop with unemployment consultants from Estonia. The workshop’s objectives were identifying user needs and creating novel XAI interfaces for the used AI system. As a result of our user-centered design approach, consultants were able to develop AI interface prototypes that would support them in creating success stories for their clients by getting detailed feedback and suggestions. We present a discussion on the value of co-creative design methods with end-users working in the public sector to improve AI application design and provide a summary of recommendations for practitioners and researchers working on AI systems in the public sector."
                ],
                "genre": [
                    "proceeding"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Explaining It Your Way - Findings from a Co-Creative Design Workshop on Designing XAI Applications with AI End-Users from the Public Sector"
                ],
                "creationdate": [
                    "20240511"
                ],
                "author": [
                    "Weitz, Katharina ; Schlagowski, Ruben ; André, Elisabeth ; Männiste, Maris ; George, Ceenu"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    "acm"
                ],
                "recordid": [
                    "cdi_acm_books_10_1145_3613904_3642563"
                ],
                "addsrcrecordid": [
                    "eNqNkM1Kw0AUhQdEUGrXbu_STepN5i-zLLHVQkFBS3UVMtOZdmibCZn49wy-tCnmAVxd-DjnwP0IuU5xkqaM31KRUoVsQgXLuKBnZKykyhmiREoRL8g4Rq-Rcy5ZlrFL8jP7ag6Vr329hUUHb-G9hXX1DQnMfb3paQTXhiNUUISkaG3V-Q8Ldzb6bQ3r0O7jLjQQ6gGdZl6nC5g2zcGbPhzqCJ--20EPZ_UmWUXbDpPdzsLTu-5z8GxNF9orcu6qQ7Tj4Y7Iaj57KR6S5eP9opgukyoTeZcIaTa5Q8dzm2kqHCpExTPlhJQ8Q8odTyk1SmuLpv9eS-0UdRRZbgwXho7I5G-3MsdSh7CPZYrlyV85-CsHf6VuvXV94eafBfoLPvRwqQ"
                ],
                "sourcerecordid": [
                    "acm_books_10_1145_3613904_3642563"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a268t-67cd8f0f58e2b36f09009529f67752035f5133c9bbe0c798b7bf93f3048cc56c3"
                ],
                "sourcetype": [
                    "Publisher"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.047590997"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:51:32&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm&rft_val_fmt=info:ofi/fmt:kev:mtx:book&rft.genre=proceeding&rft.atitle=Explaining+It+Your+Way+-+Findings+from+a+Co-Creative+Design+Workshop+on+Designing+XAI+Applications+with+AI+End-Users+from+the+Public+Sector&rft.btitle=Proceedings+of+the+2024+CHI+Conference+on+Human+Factors+in+Computing+Systems&rft.au=Weitz%2C+Katharina&rft.date=2024-05-11&rft.spage=1&rft.epage=14&rft.pages=1-14&rft.isbn=9798400703300&rft_id=info:doi/10.1145%2F3613904.3642563&rft.pub=ACM&rft.place=New+York%2C+NY%2C+USA&rft_dat=<acm>acm_books_10_1145_3613904_3642563</acm>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_acm_books_10_1145_3613904_3642563"
    },
    {
        "pnx": {
            "links": {
                "linktohtml": [
                    "$$Uhttps://login.libproxy.uregina.ca:8443/login?&url=https://www.sciencedirect.com/science/article/pii/S1071581923000150$$EHTML$$P50$$Gelsevier$$H"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "•The XAI interface for BCI experts is proposed in a user-centered view.•A pragmatic approach for designing a user-centered explanation is proposed.•Design requirements are derived in the user's context and domain knowledge.•The pragmatic explanation increases the contextual understanding of domain experts.•Design strategy is required to deliver explanatory information for a novice on XAI.\nDomain experts utilize a decision-support system depending on an artificial intelligence (AI) algorithm. Likewise, researchers in brain-computer interface (BCI) have recently employed deep learning (DL) algorithms for decoding and analyzing neural signals. Despite its outstanding performance, the BCI technology with the DLs has pointed out that it has a potential problem of low transparency due to algorithmic complexity of the models. On this problem, explainable artificial intelligence (XAI) can be a solution to make an AI algorithm and its decisions more interpretable. However, the explanation from the XAI has been emphasized that it should be designed corresponding with the user's different expectations which are contextually variable. Thus, our study aims to propose an explanation interface for the BCI expert under Pragmatism structuralizing an explanation with scientific knowledge in a contrastive manner. For this work, we conduct a contextual design process with five BCI experts, specifically conducting a contextual inquiry and work modeling to extract design requirements from their expertise in their work environment; next, designing and evaluating an interactive prototype of the explanation interface. The results indicated that our prototype has the advantages of increasing contextual understanding and intuitive interface design. Yet, there were also challenges on the explanation for novice users without prior knowledge on the XAI and objective understanding of the AI model with enough interpretability. This study contributes to providing a theoretical framework based on Pragmatism and designing a user-centered XAI system for domain experts in a specific context."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-8884-3437"
                ],
                "creator": [
                    "Kim, Sangyeon",
                    "Choo, Sanghyun",
                    "Park, Donghyun",
                    "Park, Hoonseok",
                    "Nam, Chang S.",
                    "Jung, Jae-Yoon",
                    "Lee, Sangwon"
                ],
                "title": [
                    "Designing an XAI interface for BCI experts: A contextual design for pragmatic explanation interface based on domain knowledge in a specific context",
                    "International journal of human-computer studies"
                ],
                "subject": [
                    "Pragmatism"
                ],
                "issn": [
                    "1071-5819",
                    "1095-9300"
                ],
                "fulltext": [
                    "true"
                ],
                "general": [
                    "Elsevier Ltd"
                ],
                "startdate": [
                    "202306"
                ],
                "enddate": [
                    "202306"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNp9kE1OwzAQhS0EEqVwAja-QIodN0mNxKKUApUqsQGJneXY4-CQOpEdoJyDC-O0XbBiNX_vexo9hC4pmVBC86t6Yus3FSYpSVncMEL4ERpRwrOEx-F46AuaZDPKT9FZCDUhpJgSMkI_dxBs5ayrsHT4db7C1vXgjVSATevx7WKFYduB78M1nmPVxuu2_5AN1jtwJ-q8rDayt2qQNtLFtnV_jEoZQOO40u1GWoffXfvVgK4garDEoQNlTaQP7ufoxMgmwMWhjtHL_fJ58Zisnx5Wi_k6UYywPtGZLDNemhnPU06N5tpwzpXWmvKUUAJpCllBATiUnILWSoHMGCumWT7LqWRjxPa-yrcheDCi83Yj_begRAy5ilrschVDrmKfa6Ru9hTE1z4teBGUBadAWw-qF7q1__K_FVqFDg"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2023"
                ],
                "creatorcontrib": [
                    "Kim, Sangyeon",
                    "Choo, Sanghyun",
                    "Park, Donghyun",
                    "Park, Hoonseok",
                    "Nam, Chang S.",
                    "Jung, Jae-Yoon",
                    "Lee, Sangwon"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "•The XAI interface for BCI experts is proposed in a user-centered view.•A pragmatic approach for designing a user-centered explanation is proposed.•Design requirements are derived in the user's context and domain knowledge.•The pragmatic explanation increases the contextual understanding of domain experts.•Design strategy is required to deliver explanatory information for a novice on XAI.\nDomain experts utilize a decision-support system depending on an artificial intelligence (AI) algorithm. Likewise, researchers in brain-computer interface (BCI) have recently employed deep learning (DL) algorithms for decoding and analyzing neural signals. Despite its outstanding performance, the BCI technology with the DLs has pointed out that it has a potential problem of low transparency due to algorithmic complexity of the models. On this problem, explainable artificial intelligence (XAI) can be a solution to make an AI algorithm and its decisions more interpretable. However, the explanation from the XAI has been emphasized that it should be designed corresponding with the user's different expectations which are contextually variable. Thus, our study aims to propose an explanation interface for the BCI expert under Pragmatism structuralizing an explanation with scientific knowledge in a contrastive manner. For this work, we conduct a contextual design process with five BCI experts, specifically conducting a contextual inquiry and work modeling to extract design requirements from their expertise in their work environment; next, designing and evaluating an interactive prototype of the explanation interface. The results indicated that our prototype has the advantages of increasing contextual understanding and intuitive interface design. Yet, there were also challenges on the explanation for novice users without prior knowledge on the XAI and objective understanding of the AI model with enough interpretability. This study contributes to providing a theoretical framework based on Pragmatism and designing a user-centered XAI system for domain experts in a specific context."
                ],
                "publisher": [
                    "Elsevier Ltd"
                ],
                "ispartof": [
                    "International journal of human-computer studies, 2023-06, Vol.174, p.103009, Article 103009"
                ],
                "identifier": [
                    "ISSN: 1071-5819",
                    "EISSN: 1095-9300",
                    "DOI: 10.1016/j.ijhcs.2023.103009"
                ],
                "rights": [
                    "2023 Elsevier Ltd"
                ],
                "snippet": [
                    "•The XAI interface for BCI experts is proposed in a user-centered view.•A pragmatic approach for designing a user-centered explanation is proposed..."
                ],
                "creator": [
                    "Kim, Sangyeon ; Choo, Sanghyun ; Park, Donghyun ; Park, Hoonseok ; Nam, Chang S. ; Jung, Jae-Yoon ; Lee, Sangwon"
                ],
                "title": [
                    "Designing an XAI interface for BCI experts: A contextual design for pragmatic explanation interface based on domain knowledge in a specific context"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "Elsevier ScienceDirect Journals Complete"
                ],
                "language": [
                    "eng"
                ],
                "subject": [
                    "Pragmatism"
                ],
                "keyword": [
                    "Brain-computer interface (BCI) ;  Contextual design ;  Expert system ;  Explainable artificial intelligence (XAI) ;  Explanation interface"
                ]
            },
            "facets": {
                "creationdate": [
                    "2023"
                ],
                "creatorcontrib": [
                    "Kim, Sangyeon",
                    "Choo, Sanghyun",
                    "Park, Donghyun",
                    "Park, Hoonseok",
                    "Nam, Chang S.",
                    "Jung, Jae-Yoon",
                    "Lee, Sangwon"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-c303t-d5ab59bf896291fd9df999cddd192010e22e571ee9eb91eddccea5337456861a3"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "International journal of human-computer studies"
                ],
                "topic": [
                    "Pragmatism"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "pub": [
                    "Elsevier Ltd"
                ],
                "doi": [
                    "10.1016/j.ijhcs.2023.103009"
                ],
                "au": [
                    "Kim, Sangyeon",
                    "Choo, Sanghyun",
                    "Park, Donghyun",
                    "Park, Hoonseok",
                    "Nam, Chang S.",
                    "Jung, Jae-Yoon",
                    "Lee, Sangwon"
                ],
                "atitle": [
                    "Designing an XAI interface for BCI experts: A contextual design for pragmatic explanation interface based on domain knowledge in a specific context"
                ],
                "date": [
                    "2023-06"
                ],
                "risdate": [
                    "2023"
                ],
                "volume": [
                    "174"
                ],
                "spage": [
                    "103009"
                ],
                "pages": [
                    "103009-"
                ],
                "artnum": [
                    "103009"
                ],
                "eissn": [
                    "1095-9300"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-8884-3437"
                ],
                "issn": [
                    "1071-5819"
                ],
                "abstract": [
                    "•The XAI interface for BCI experts is proposed in a user-centered view.•A pragmatic approach for designing a user-centered explanation is proposed.•Design requirements are derived in the user's context and domain knowledge.•The pragmatic explanation increases the contextual understanding of domain experts.•Design strategy is required to deliver explanatory information for a novice on XAI.\nDomain experts utilize a decision-support system depending on an artificial intelligence (AI) algorithm. Likewise, researchers in brain-computer interface (BCI) have recently employed deep learning (DL) algorithms for decoding and analyzing neural signals. Despite its outstanding performance, the BCI technology with the DLs has pointed out that it has a potential problem of low transparency due to algorithmic complexity of the models. On this problem, explainable artificial intelligence (XAI) can be a solution to make an AI algorithm and its decisions more interpretable. However, the explanation from the XAI has been emphasized that it should be designed corresponding with the user's different expectations which are contextually variable. Thus, our study aims to propose an explanation interface for the BCI expert under Pragmatism structuralizing an explanation with scientific knowledge in a contrastive manner. For this work, we conduct a contextual design process with five BCI experts, specifically conducting a contextual inquiry and work modeling to extract design requirements from their expertise in their work environment; next, designing and evaluating an interactive prototype of the explanation interface. The results indicated that our prototype has the advantages of increasing contextual understanding and intuitive interface design. Yet, there were also challenges on the explanation for novice users without prior knowledge on the XAI and objective understanding of the AI model with enough interpretability. This study contributes to providing a theoretical framework based on Pragmatism and designing a user-centered XAI system for domain experts in a specific context."
                ],
                "jtitle": [
                    "International journal of human-computer studies"
                ],
                "genre": [
                    "article"
                ]
            },
            "sort": {
                "title": [
                    "Designing an XAI interface for BCI experts: A contextual design for pragmatic explanation interface based on domain knowledge in a specific context"
                ],
                "creationdate": [
                    "202306"
                ],
                "author": [
                    "Kim, Sangyeon ; Choo, Sanghyun ; Park, Donghyun ; Park, Hoonseok ; Nam, Chang S. ; Jung, Jae-Yoon ; Lee, Sangwon"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "elsid": [
                    "S1071581923000150"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "elsevier_cross"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_1016_j_ijhcs_2023_103009"
                ],
                "addsrcrecordid": [
                    "eNp9kE1OwzAQhS0EEqVwAja-QIodN0mNxKKUApUqsQGJneXY4-CQOpEdoJyDC-O0XbBiNX_vexo9hC4pmVBC86t6Yus3FSYpSVncMEL4ERpRwrOEx-F46AuaZDPKT9FZCDUhpJgSMkI_dxBs5ayrsHT4db7C1vXgjVSATevx7WKFYduB78M1nmPVxuu2_5AN1jtwJ-q8rDayt2qQNtLFtnV_jEoZQOO40u1GWoffXfvVgK4garDEoQNlTaQP7ufoxMgmwMWhjtHL_fJ58Zisnx5Wi_k6UYywPtGZLDNemhnPU06N5tpwzpXWmvKUUAJpCllBATiUnILWSoHMGCumWT7LqWRjxPa-yrcheDCi83Yj_begRAy5ilrschVDrmKfa6Ru9hTE1z4teBGUBadAWw-qF7q1__K_FVqFDg"
                ],
                "sourcerecordid": [
                    "S1071581923000150"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-c303t-d5ab59bf896291fd9df999cddd192010e22e571ee9eb91eddccea5337456861a3"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.041526772"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:51:32&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-elsevier_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=Designing+an+XAI+interface+for+BCI+experts%3A+A+contextual+design+for+pragmatic+explanation+interface+based+on+domain+knowledge+in+a+specific+context&rft.jtitle=International+journal+of+human-computer+studies&rft.au=Kim%2C+Sangyeon&rft.date=2023-06&rft.volume=174&rft.spage=103009&rft.pages=103009-&rft.artnum=103009&rft.issn=1071-5819&rft.eissn=1095-9300&rft_id=info:doi/10.1016%2Fj.ijhcs.2023.103009&rft.pub=Elsevier+Ltd&rft_dat=<elsevier_cross>S1071581923000150</elsevier_cross>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-c303t-d5ab59bf896291fd9df999cddd192010e22e571ee9eb91eddccea5337456861a3"
                ],
                "citedby": [
                    "FETCH-LOGICAL-c303t-d5ab59bf896291fd9df999cddd192010e22e571ee9eb91eddccea5337456861a3"
                ]
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_1016_j_ijhcs_2023_103009"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3322276.3322318$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems."
                ],
                "creator": [
                    "Robb, David A.",
                    "Lopes, José",
                    "Padilla, Stefano",
                    "Laskov, Atanas",
                    "Chiyah Garcia, Francisco J.",
                    "Liu, Xingkun",
                    "Scharff Willners, Jonatan",
                    "Valeyrie, Nicolas",
                    "Lohan, Katrin",
                    "Lane, David",
                    "Patron, Pedro",
                    "Petillot, Yvan",
                    "Chantler, Mike J.",
                    "Hastie, Helen"
                ],
                "title": [
                    "Exploring Interaction with Remote Autonomous Systems using Conversational Agents",
                    "Proceedings of the 2019 on Designing Interactive Systems Conference"
                ],
                "isbn": [
                    "9781450358507",
                    "1450358500"
                ],
                "fulltext": [
                    "true"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20190618"
                ],
                "enddate": [
                    "20190618"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    ""
                ],
                "recordid": [
                    "eNqNkMtKxDAUhgMiKGPXbrN005pL07TLUkYdGFC8rEOSScZqm0iTenl7U-wDCAf-xfm_w-ED4BKjAuOSXVNKCOFVsSTF9QnIGl6nBaKsZoifgSyEN4QQqSuUgHPwsP3-GPzUuyPcuWgmqWPvHfzq4yt8NKOPBrZz9M6Pfg7w6SdEMwY4hwXovPs0U5ALIQfYHo2L4QKcWjkEk625AS832-fuLt_f3-66dp9LinjMubW4UQzXGluqNFdMyqZhyJrSsINKvzJJKnpQ3JKScKaZVQ3VZRostUF0A67-7ko9CuX9exAYiUWCWCWIVUKqFv-sCjX1xtJf9pBe2g"
                ],
                "scope": [
                    ""
                ],
                "creationdate": [
                    "2019"
                ],
                "creatorcontrib": [
                    "Robb, David A.",
                    "Lopes, José",
                    "Padilla, Stefano",
                    "Laskov, Atanas",
                    "Chiyah Garcia, Francisco J.",
                    "Liu, Xingkun",
                    "Scharff Willners, Jonatan",
                    "Valeyrie, Nicolas",
                    "Lohan, Katrin",
                    "Lane, David",
                    "Patron, Pedro",
                    "Petillot, Yvan",
                    "Chantler, Mike J.",
                    "Hastie, Helen"
                ],
                "rsrctype": [
                    "conference_proceeding"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems."
                ],
                "publisher": [
                    "New York, NY, USA: ACM"
                ],
                "ispartof": [
                    "Proceedings of the 2019 on Designing Interactive Systems Conference, 2019, p.1543-1556"
                ],
                "identifier": [
                    "ISBN: 9781450358507",
                    "ISBN: 1450358500",
                    "DOI: 10.1145/3322276.3322318"
                ],
                "rights": [
                    "2019 Owner/Author"
                ],
                "snippet": [
                    ".... As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent..."
                ],
                "creator": [
                    "Robb, David A. ; Lopes, José ; Padilla, Stefano ; Laskov, Atanas ; Chiyah Garcia, Francisco J. ; Liu, Xingkun ; Scharff Willners, Jonatan ; Valeyrie, Nicolas ; Lohan, Katrin ; Lane, David ; Patron, Pedro ; Petillot, Yvan ; Chantler, Mike J. ; Hastie, Helen"
                ],
                "title": [
                    "Exploring Interaction with Remote Autonomous Systems using Conversational Agents"
                ],
                "type": [
                    "conference_proceeding"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "keyword": [
                    "Computer systems organization -- Embedded and cyber-physical systems -- Robotics -- Robotic autonomy ;  Human-centered computing -- Human computer interaction (HCI) -- Interaction paradigms -- Graphical user interfaces ;  Human-centered computing -- Human computer interaction (HCI) -- Interaction paradigms -- Natural language interfaces"
                ]
            },
            "facets": {
                "creationdate": [
                    "2019"
                ],
                "creatorcontrib": [
                    "Robb, David A.",
                    "Lopes, José",
                    "Padilla, Stefano",
                    "Laskov, Atanas",
                    "Chiyah Garcia, Francisco J.",
                    "Liu, Xingkun",
                    "Scharff Willners, Jonatan",
                    "Valeyrie, Nicolas",
                    "Lohan, Katrin",
                    "Lane, David",
                    "Patron, Pedro",
                    "Petillot, Yvan",
                    "Chantler, Mike J.",
                    "Hastie, Helen"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a307t-7ff19b518c1f3bc7b5aa9950fe4e5db5075a263db7f24275c5fb93c43c41ace03"
                ],
                "frbrtype": [
                    "5"
                ],
                "prefilter": [
                    "conference_proceedings"
                ],
                "rsrctype": [
                    "conference_proceedings"
                ]
            },
            "addata": {
                "format": [
                    "book"
                ],
                "ristype": [
                    "CONF"
                ],
                "cop": [
                    "New York, NY, USA"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3322276.3322318"
                ],
                "tpages": [
                    "14"
                ],
                "au": [
                    "Robb, David A.",
                    "Lopes, José",
                    "Padilla, Stefano",
                    "Laskov, Atanas",
                    "Chiyah Garcia, Francisco J.",
                    "Liu, Xingkun",
                    "Scharff Willners, Jonatan",
                    "Valeyrie, Nicolas",
                    "Lohan, Katrin",
                    "Lane, David",
                    "Patron, Pedro",
                    "Petillot, Yvan",
                    "Chantler, Mike J.",
                    "Hastie, Helen"
                ],
                "btitle": [
                    "Proceedings of the 2019 on Designing Interactive Systems Conference"
                ],
                "atitle": [
                    "Exploring Interaction with Remote Autonomous Systems using Conversational Agents"
                ],
                "date": [
                    "2019-06-18"
                ],
                "risdate": [
                    "2019"
                ],
                "spage": [
                    "1543"
                ],
                "epage": [
                    "1556"
                ],
                "pages": [
                    "1543-1556"
                ],
                "isbn": [
                    "9781450358507",
                    "1450358500"
                ],
                "abstract": [
                    "Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems."
                ],
                "genre": [
                    "proceeding"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Exploring Interaction with Remote Autonomous Systems using Conversational Agents"
                ],
                "creationdate": [
                    "20190618"
                ],
                "author": [
                    "Robb, David A. ; Lopes, José ; Padilla, Stefano ; Laskov, Atanas ; Chiyah Garcia, Francisco J. ; Liu, Xingkun ; Scharff Willners, Jonatan ; Valeyrie, Nicolas ; Lohan, Katrin ; Lane, David ; Patron, Pedro ; Petillot, Yvan ; Chantler, Mike J. ; Hastie, Helen"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "conference_proceeding"
                ],
                "sourceid": [
                    "acm"
                ],
                "recordid": [
                    "cdi_acm_books_10_1145_3322276_3322318_brief"
                ],
                "addsrcrecordid": [
                    "eNqNkMtKxDAUhgMiKGPXbrN005pL07TLUkYdGFC8rEOSScZqm0iTenl7U-wDCAf-xfm_w-ED4BKjAuOSXVNKCOFVsSTF9QnIGl6nBaKsZoifgSyEN4QQqSuUgHPwsP3-GPzUuyPcuWgmqWPvHfzq4yt8NKOPBrZz9M6Pfg7w6SdEMwY4hwXovPs0U5ALIQfYHo2L4QKcWjkEk625AS832-fuLt_f3-66dp9LinjMubW4UQzXGluqNFdMyqZhyJrSsINKvzJJKnpQ3JKScKaZVQ3VZRostUF0A67-7ko9CuX9exAYiUWCWCWIVUKqFv-sCjX1xtJf9pBe2g"
                ],
                "sourcerecordid": [
                    "acm_books_10_1145_3322276_3322318"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a307t-7ff19b518c1f3bc7b5aa9950fe4e5db5075a263db7f24275c5fb93c43c41ace03"
                ],
                "sourcetype": [
                    "Publisher"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.031062104"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:51:32&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm&rft_val_fmt=info:ofi/fmt:kev:mtx:book&rft.genre=proceeding&rft.atitle=Exploring+Interaction+with+Remote+Autonomous+Systems+using+Conversational+Agents&rft.btitle=Proceedings+of+the+2019+on+Designing+Interactive+Systems+Conference&rft.au=Robb%2C+David+A.&rft.date=2019-06-18&rft.spage=1543&rft.epage=1556&rft.pages=1543-1556&rft.isbn=9781450358507&rft_id=info:doi/10.1145%2F3322276.3322318&rft.pub=ACM&rft.place=New+York%2C+NY%2C+USA&rft_dat=<acm>acm_books_10_1145_3322276_3322318</acm>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_acm_books_10_1145_3322276_3322318_brief"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3641022$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9378-0872",
                    "https://orcid.org/0000-0002-8369-3847",
                    "https://orcid.org/0009-0002-2358-2341",
                    "https://orcid.org/0000-0001-6750-0876",
                    "https://orcid.org/0000-0002-2644-4422",
                    "https://orcid.org/0009-0002-3081-5617"
                ],
                "creator": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making",
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "issn": [
                    "2573-0142",
                    "2573-0142"
                ],
                "fulltext": [
                    "true"
                ],
                "addtitle": [
                    "ACM PACMHCI"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20240423"
                ],
                "enddate": [
                    "20240423"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNpNj89Lw0AQhRdRsLTFu6fcPK3OTLKb3WOpPxpo8VLBW9hsZjVqkpLVg_-9Ka3i6X2P-Rh4QlwgXCNm6ibVGQLRiZiQylMJmNHpPz4X8xjfAACNAmVpIvLtKydFu3P-M-nDnngIPJbnRZH0XbL6al0nR75l38Sm7-TGvTfdy0ycBfcReX7MqXi6v9suV3L9-FAsF2vp0BJJwsAMHExQSDUz6pCqYK2yqkadegpOs_aVN2CqmjMAMp6r3OaeoVKQTsXV4a8f-hgHDuVuaFo3fJcI5X5yeZw8mpcH0_n2T_o9_gDOr04e"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "publisher": [
                    "New York, NY, USA: ACM"
                ],
                "ispartof": [
                    "Proceedings of the ACM on human-computer interaction, 2024-04, Vol.8 (CSCW1), p.1-39, Article 183"
                ],
                "identifier": [
                    "ISSN: 2573-0142",
                    "EISSN: 2573-0142",
                    "DOI: 10.1145/3641022"
                ],
                "rights": [
                    "Owner/Author"
                ],
                "snippet": [
                    "...-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI..."
                ],
                "creator": [
                    "Morrison, Katelyn ; Spitzer, Philipp ; Turri, Violet ; Feng, Michelle ; Kühl, Niklas ; Perer, Adam"
                ],
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "ACM Digital Library Complete (OCUL)"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "keyword": [
                    "Human-centered computing ;  Human-centered computing / Collaborative and social computing ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing design and evaluation methods ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing theory, concepts and paradigms ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing theory, concepts and paradigms / Computer supported cooperative work ;  Human-centered computing / Human computer interaction (HCI) ;  Human-centered computing / Human computer interaction (HCI) / Empirical studies in HCI ;  Human-centered computing / Human computer interaction (HCI) / Interaction paradigms ;  Human-centered computing / Human computer interaction (HCI) / Interaction paradigms / Collaborative interaction ;  Social and professional topics ;  Social and professional topics / Professional topics ;  Social and professional topics / Professional topics / Computing and business ;  Social and professional topics / Professional topics / Computing and business / Computer supported cooperative work"
                ]
            },
            "facets": {
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "cop": [
                    "New York, NY, USA"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3641022"
                ],
                "tpages": [
                    "39"
                ],
                "au": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "atitle": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "stitle": [
                    "ACM PACMHCI"
                ],
                "date": [
                    "2024-04-23"
                ],
                "risdate": [
                    "2024"
                ],
                "volume": [
                    "8"
                ],
                "issue": [
                    "CSCW1"
                ],
                "spage": [
                    "1"
                ],
                "epage": [
                    "39"
                ],
                "pages": [
                    "1-39"
                ],
                "artnum": [
                    "183"
                ],
                "eissn": [
                    "2573-0142"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9378-0872",
                    "https://orcid.org/0000-0002-8369-3847",
                    "https://orcid.org/0009-0002-2358-2341",
                    "https://orcid.org/0000-0001-6750-0876",
                    "https://orcid.org/0000-0002-2644-4422",
                    "https://orcid.org/0009-0002-3081-5617"
                ],
                "issn": [
                    "2573-0142"
                ],
                "abstract": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "jtitle": [
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "creationdate": [
                    "20240423"
                ],
                "author": [
                    "Morrison, Katelyn ; Spitzer, Philipp ; Turri, Violet ; Feng, Michelle ; Kühl, Niklas ; Perer, Adam"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "acm_cross"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_1145_3641022"
                ],
                "addsrcrecordid": [
                    "eNpNj89Lw0AQhRdRsLTFu6fcPK3OTLKb3WOpPxpo8VLBW9hsZjVqkpLVg_-9Ka3i6X2P-Rh4QlwgXCNm6ibVGQLRiZiQylMJmNHpPz4X8xjfAACNAmVpIvLtKydFu3P-M-nDnngIPJbnRZH0XbL6al0nR75l38Sm7-TGvTfdy0ycBfcReX7MqXi6v9suV3L9-FAsF2vp0BJJwsAMHExQSDUz6pCqYK2yqkadegpOs_aVN2CqmjMAMp6r3OaeoVKQTsXV4a8f-hgHDuVuaFo3fJcI5X5yeZw8mpcH0_n2T_o9_gDOr04e"
                ],
                "sourcerecordid": [
                    "3641022"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.07"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 17:57:37&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=The+Impact+of+Imperfect+XAI+on+Human-AI+Decision-Making&rft.jtitle=Proceedings+of+the+ACM+on+human-computer+interaction&rft.au=Morrison%2C+Katelyn&rft.date=2024-04-23&rft.volume=8&rft.issue=CSCW1&rft.spage=1&rft.epage=39&rft.pages=1-39&rft.artnum=183&rft.issn=2573-0142&rft.eissn=2573-0142&rft_id=info:doi/10.1145%2F3641022&rft.pub=ACM&rft.place=New+York%2C+NY%2C+USA&rft.stitle=ACM+PACMHCI&rft_dat=<acm_cross>3641022</acm_cross>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "citedby": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ]
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_1145_3641022"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3641022$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9378-0872",
                    "https://orcid.org/0000-0002-8369-3847",
                    "https://orcid.org/0009-0002-2358-2341",
                    "https://orcid.org/0000-0001-6750-0876",
                    "https://orcid.org/0000-0002-2644-4422",
                    "https://orcid.org/0009-0002-3081-5617"
                ],
                "creator": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making",
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "issn": [
                    "2573-0142",
                    "2573-0142"
                ],
                "fulltext": [
                    "true"
                ],
                "addtitle": [
                    "ACM PACMHCI"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20240423"
                ],
                "enddate": [
                    "20240423"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNpNj89Lw0AQhRdRsLTFu6fcPK3OTLKb3WOpPxpo8VLBW9hsZjVqkpLVg_-9Ka3i6X2P-Rh4QlwgXCNm6ibVGQLRiZiQylMJmNHpPz4X8xjfAACNAmVpIvLtKydFu3P-M-nDnngIPJbnRZH0XbL6al0nR75l38Sm7-TGvTfdy0ycBfcReX7MqXi6v9suV3L9-FAsF2vp0BJJwsAMHExQSDUz6pCqYK2yqkadegpOs_aVN2CqmjMAMp6r3OaeoVKQTsXV4a8f-hgHDuVuaFo3fJcI5X5yeZw8mpcH0_n2T_o9_gDOr04e"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "publisher": [
                    "New York, NY, USA: ACM"
                ],
                "ispartof": [
                    "Proceedings of the ACM on human-computer interaction, 2024-04, Vol.8 (CSCW1), p.1-39, Article 183"
                ],
                "identifier": [
                    "ISSN: 2573-0142",
                    "EISSN: 2573-0142",
                    "DOI: 10.1145/3641022"
                ],
                "rights": [
                    "Owner/Author"
                ],
                "snippet": [
                    "...-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI..."
                ],
                "creator": [
                    "Morrison, Katelyn ; Spitzer, Philipp ; Turri, Violet ; Feng, Michelle ; Kühl, Niklas ; Perer, Adam"
                ],
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "keyword": [
                    "Human-centered computing ;  Human-centered computing / Collaborative and social computing ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing design and evaluation methods ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing theory, concepts and paradigms ;  Human-centered computing / Collaborative and social computing / Collaborative and social computing theory, concepts and paradigms / Computer supported cooperative work ;  Human-centered computing / Human computer interaction (HCI) ;  Human-centered computing / Human computer interaction (HCI) / Empirical studies in HCI ;  Human-centered computing / Human computer interaction (HCI) / Interaction paradigms ;  Human-centered computing / Human computer interaction (HCI) / Interaction paradigms / Collaborative interaction ;  Social and professional topics ;  Social and professional topics / Professional topics ;  Social and professional topics / Professional topics / Computing and business ;  Social and professional topics / Professional topics / Computing and business / Computer supported cooperative work"
                ]
            },
            "facets": {
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "cop": [
                    "New York, NY, USA"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3641022"
                ],
                "tpages": [
                    "39"
                ],
                "au": [
                    "Morrison, Katelyn",
                    "Spitzer, Philipp",
                    "Turri, Violet",
                    "Feng, Michelle",
                    "Kühl, Niklas",
                    "Perer, Adam"
                ],
                "atitle": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "stitle": [
                    "ACM PACMHCI"
                ],
                "date": [
                    "2024-04-23"
                ],
                "risdate": [
                    "2024"
                ],
                "volume": [
                    "8"
                ],
                "issue": [
                    "CSCW1"
                ],
                "spage": [
                    "1"
                ],
                "epage": [
                    "39"
                ],
                "pages": [
                    "1-39"
                ],
                "artnum": [
                    "183"
                ],
                "eissn": [
                    "2573-0142"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-9378-0872",
                    "https://orcid.org/0000-0002-8369-3847",
                    "https://orcid.org/0009-0002-2358-2341",
                    "https://orcid.org/0000-0001-6750-0876",
                    "https://orcid.org/0000-0002-2644-4422",
                    "https://orcid.org/0009-0002-3081-5617"
                ],
                "issn": [
                    "2573-0142"
                ],
                "abstract": [
                    "Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
                ],
                "jtitle": [
                    "Proceedings of the ACM on human-computer interaction"
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "The Impact of Imperfect XAI on Human-AI Decision-Making"
                ],
                "creationdate": [
                    "20240423"
                ],
                "author": [
                    "Morrison, Katelyn ; Spitzer, Philipp ; Turri, Violet ; Feng, Michelle ; Kühl, Niklas ; Perer, Adam"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "acm_cross"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_1145_3641022"
                ],
                "addsrcrecordid": [
                    "eNpNj89Lw0AQhRdRsLTFu6fcPK3OTLKb3WOpPxpo8VLBW9hsZjVqkpLVg_-9Ka3i6X2P-Rh4QlwgXCNm6ibVGQLRiZiQylMJmNHpPz4X8xjfAACNAmVpIvLtKydFu3P-M-nDnngIPJbnRZH0XbL6al0nR75l38Sm7-TGvTfdy0ycBfcReX7MqXi6v9suV3L9-FAsF2vp0BJJwsAMHExQSDUz6pCqYK2yqkadegpOs_aVN2CqmjMAMp6r3OaeoVKQTsXV4a8f-hgHDuVuaFo3fJcI5X5yeZw8mpcH0_n2T_o9_gDOr04e"
                ],
                "sourcerecordid": [
                    "3641022"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.07"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 18:06:31&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=The+Impact+of+Imperfect+XAI+on+Human-AI+Decision-Making&rft.jtitle=Proceedings+of+the+ACM+on+human-computer+interaction&rft.au=Morrison%2C+Katelyn&rft.date=2024-04-23&rft.volume=8&rft.issue=CSCW1&rft.spage=1&rft.epage=39&rft.pages=1-39&rft.artnum=183&rft.issn=2573-0142&rft.eissn=2573-0142&rft_id=info:doi/10.1145%2F3641022&rft.pub=ACM&rft.place=New+York%2C+NY%2C+USA&rft.stitle=ACM+PACMHCI&rft_dat=<acm_cross>3641022</acm_cross>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ],
                "citedby": [
                    "FETCH-LOGICAL-a1922-21fee0ef8f512dee16f35f99595d163c2fa6e6cbc808bde40028ceb797ce0b503"
                ]
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_1145_3641022"
    },
    {
        "pnx": {
            "links": {
                "linktohtml": [
                    "$$Uhttps://login.libproxy.uregina.ca:8443/login?&url=https://www.sciencedirect.com/science/article/pii/S000437022100076X$$EHTML$$P50$$Gelsevier$$H"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "•Provide insights into AI-Human communication.•Define levels of explanation with identified techniques that align with AI cognitive processes.•Discuss insights into Broad eXplainable Artificial Intelligence (Broad-XAI).•Align AI explanation to human communication.\nOver the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI) and the closely aligned Interpretable Machine Learning (IML). Drivers for this growth include recent legislative changes and increased investments by industry and governments, along with increased concern from the general public. People are affected by autonomous decisions every day and the public need to understand the decision-making process to accept the outcomes. However, the vast majority of the applications of XAI/IML are focused on providing low-level ‘narrow’ explanations of how an individual decision was reached based on a particular datum. While important, these explanations rarely provide insights into an agent's: beliefs and motivations; hypotheses of other (human, animal or AI) agents' intentions; interpretation of external cultural expectations; or, processes used to generate its own explanation. Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making. This paper aims to define levels of explanation and describe how they can be integrated to create a human-aligned conversational explanation system. In so doing, this paper will survey current approaches and discuss the integration of different technologies to achieve these levels with Broad eXplainable Artificial Intelligence (Broad-XAI), and thereby move towards high-level ‘strong’ explanations."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0003-2537-0326",
                    "https://orcid.org/0000-0002-8687-4424",
                    "https://orcid.org/0000-0002-6639-6824",
                    "https://orcid.org/0000-0002-1131-3382",
                    "https://orcid.org/0000-0002-6199-9685"
                ],
                "creator": [
                    "Dazeley, Richard",
                    "Vamplew, Peter",
                    "Foale, Cameron",
                    "Young, Charlotte",
                    "Aryal, Sunil",
                    "Cruz, Francisco"
                ],
                "title": [
                    "Levels of explainable artificial intelligence for human-aligned conversational explanations",
                    "Artificial intelligence"
                ],
                "subject": [
                    "Artificial intelligence",
                    "Decision making",
                    "Machine learning"
                ],
                "issn": [
                    "0004-3702",
                    "1872-7921"
                ],
                "fulltext": [
                    "true"
                ],
                "general": [
                    "Elsevier B.V",
                    "Elsevier Science Ltd"
                ],
                "startdate": [
                    "202110"
                ],
                "enddate": [
                    "202110"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNp9kE9LAzEQxYMoWKvfwMOC56351-zuRZCiVSh40ZOHkM1ONEua1GRb9Nub7Xr2NLzhzWPeD6FrghcEE3HbL1QcrB8WFFOSV2xJlydoRuqKllVDySmaYYx5ySpMz9FFSn2WrGnIDL1v4AAuFcEU8L1zynrVOijGPGO1Va7IueCc_QCvoTAhFp_7rfKlyisPXaGDP0BMarDBZ_cxxB9VukRnRrkEV39zjt4eH15XT-XmZf28ut-UmtV4KJlgWvBWt_lvxTWlLa-B4rbLRbpKQdNqAqpSDWsNN0LXQlSag8iKMN5oNkc3U-4uhq89pEH2YR_zN0nSZU1qwXPb7OKTS8eQUgQjd9FuVfyRBMsRo-zlhFGOGOWEMZ_dTWeZEhwsRJm0HVl0NoIeZBfs_wG_-CN_Nw"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION",
                    "7SC",
                    "8FD",
                    "JQ2",
                    "L7M",
                    "L~C",
                    "L~D"
                ],
                "creationdate": [
                    "2021"
                ],
                "creatorcontrib": [
                    "Dazeley, Richard",
                    "Vamplew, Peter",
                    "Foale, Cameron",
                    "Young, Charlotte",
                    "Aryal, Sunil",
                    "Cruz, Francisco"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "•Provide insights into AI-Human communication.•Define levels of explanation with identified techniques that align with AI cognitive processes.•Discuss insights into Broad eXplainable Artificial Intelligence (Broad-XAI).•Align AI explanation to human communication.\nOver the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI) and the closely aligned Interpretable Machine Learning (IML). Drivers for this growth include recent legislative changes and increased investments by industry and governments, along with increased concern from the general public. People are affected by autonomous decisions every day and the public need to understand the decision-making process to accept the outcomes. However, the vast majority of the applications of XAI/IML are focused on providing low-level ‘narrow’ explanations of how an individual decision was reached based on a particular datum. While important, these explanations rarely provide insights into an agent's: beliefs and motivations; hypotheses of other (human, animal or AI) agents' intentions; interpretation of external cultural expectations; or, processes used to generate its own explanation. Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making. This paper aims to define levels of explanation and describe how they can be integrated to create a human-aligned conversational explanation system. In so doing, this paper will survey current approaches and discuss the integration of different technologies to achieve these levels with Broad eXplainable Artificial Intelligence (Broad-XAI), and thereby move towards high-level ‘strong’ explanations."
                ],
                "publisher": [
                    "Amsterdam: Elsevier B.V"
                ],
                "ispartof": [
                    "Artificial intelligence, 2021-10, Vol.299, p.103525, Article 103525"
                ],
                "identifier": [
                    "ISSN: 0004-3702",
                    "EISSN: 1872-7921",
                    "DOI: 10.1016/j.artint.2021.103525"
                ],
                "rights": [
                    "2021 Elsevier B.V.",
                    "Copyright Elsevier Science Ltd. Oct 2021"
                ],
                "snippet": [
                    "....•Discuss insights into Broad eXplainable Artificial Intelligence (Broad-XAI).•Align AI explanation to human communication...",
                    "Over the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI..."
                ],
                "creator": [
                    "Dazeley, Richard ; Vamplew, Peter ; Foale, Cameron ; Young, Charlotte ; Aryal, Sunil ; Cruz, Francisco"
                ],
                "title": [
                    "Levels of explainable artificial intelligence for human-aligned conversational explanations"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "Elsevier ScienceDirect Journals Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "subject": [
                    "Artificial intelligence ;  Decision making ;  Machine learning"
                ],
                "keyword": [
                    "Agents (artificial intelligence) ;  Artificial General Intelligence (AGI) ;  Broad-XAI ;  Explainable artificial intelligence ;  Explainable Artificial Intelligence (XAI) ;  Human-Computer Interaction (HCI) ;  Interpretable Machine Learning (IML)"
                ]
            },
            "facets": {
                "creationdate": [
                    "2021"
                ],
                "creatorcontrib": [
                    "Dazeley, Richard",
                    "Vamplew, Peter",
                    "Foale, Cameron",
                    "Young, Charlotte",
                    "Aryal, Sunil",
                    "Cruz, Francisco"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-c380t-363c64bcb035a4c22b48e20bd103d7ae9bc1ea7a93bf4f6c8667c4e6bf41349c3"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "Artificial intelligence"
                ],
                "topic": [
                    "Artificial intelligence",
                    "Decision making",
                    "Machine learning"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef",
                    "Computer and Information Systems Abstracts",
                    "Technology Research Database",
                    "ProQuest Computer Science Collection",
                    "Advanced Technologies Database with Aerospace",
                    "Computer and Information Systems Abstracts  Academic",
                    "Computer and Information Systems Abstracts Professional"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "cop": [
                    "Amsterdam"
                ],
                "pub": [
                    "Elsevier B.V"
                ],
                "doi": [
                    "10.1016/j.artint.2021.103525"
                ],
                "au": [
                    "Dazeley, Richard",
                    "Vamplew, Peter",
                    "Foale, Cameron",
                    "Young, Charlotte",
                    "Aryal, Sunil",
                    "Cruz, Francisco"
                ],
                "atitle": [
                    "Levels of explainable artificial intelligence for human-aligned conversational explanations"
                ],
                "date": [
                    "2021-10"
                ],
                "risdate": [
                    "2021"
                ],
                "volume": [
                    "299"
                ],
                "spage": [
                    "103525"
                ],
                "pages": [
                    "103525-"
                ],
                "artnum": [
                    "103525"
                ],
                "eissn": [
                    "1872-7921"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0003-2537-0326",
                    "https://orcid.org/0000-0002-8687-4424",
                    "https://orcid.org/0000-0002-6639-6824",
                    "https://orcid.org/0000-0002-1131-3382",
                    "https://orcid.org/0000-0002-6199-9685"
                ],
                "issn": [
                    "0004-3702"
                ],
                "abstract": [
                    "•Provide insights into AI-Human communication.•Define levels of explanation with identified techniques that align with AI cognitive processes.•Discuss insights into Broad eXplainable Artificial Intelligence (Broad-XAI).•Align AI explanation to human communication.\nOver the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI) and the closely aligned Interpretable Machine Learning (IML). Drivers for this growth include recent legislative changes and increased investments by industry and governments, along with increased concern from the general public. People are affected by autonomous decisions every day and the public need to understand the decision-making process to accept the outcomes. However, the vast majority of the applications of XAI/IML are focused on providing low-level ‘narrow’ explanations of how an individual decision was reached based on a particular datum. While important, these explanations rarely provide insights into an agent's: beliefs and motivations; hypotheses of other (human, animal or AI) agents' intentions; interpretation of external cultural expectations; or, processes used to generate its own explanation. Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making. This paper aims to define levels of explanation and describe how they can be integrated to create a human-aligned conversational explanation system. In so doing, this paper will survey current approaches and discuss the integration of different technologies to achieve these levels with Broad eXplainable Artificial Intelligence (Broad-XAI), and thereby move towards high-level ‘strong’ explanations."
                ],
                "jtitle": [
                    "Artificial intelligence"
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Levels of explainable artificial intelligence for human-aligned conversational explanations"
                ],
                "creationdate": [
                    "202110"
                ],
                "author": [
                    "Dazeley, Richard ; Vamplew, Peter ; Foale, Cameron ; Young, Charlotte ; Aryal, Sunil ; Cruz, Francisco"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "elsid": [
                    "S000437022100076X"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "proquest_cross"
                ],
                "recordid": [
                    "cdi_proquest_journals_2581864003"
                ],
                "addsrcrecordid": [
                    "eNp9kE9LAzEQxYMoWKvfwMOC56351-zuRZCiVSh40ZOHkM1ONEua1GRb9Nub7Xr2NLzhzWPeD6FrghcEE3HbL1QcrB8WFFOSV2xJlydoRuqKllVDySmaYYx5ySpMz9FFSn2WrGnIDL1v4AAuFcEU8L1zynrVOijGPGO1Va7IueCc_QCvoTAhFp_7rfKlyisPXaGDP0BMarDBZ_cxxB9VukRnRrkEV39zjt4eH15XT-XmZf28ut-UmtV4KJlgWvBWt_lvxTWlLa-B4rbLRbpKQdNqAqpSDWsNN0LXQlSag8iKMN5oNkc3U-4uhq89pEH2YR_zN0nSZU1qwXPb7OKTS8eQUgQjd9FuVfyRBMsRo-zlhFGOGOWEMZ_dTWeZEhwsRJm0HVl0NoIeZBfs_wG_-CN_Nw"
                ],
                "sourcerecordid": [
                    "2581864003"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-c380t-363c64bcb035a4c22b48e20bd103d7ae9bc1ea7a93bf4f6c8667c4e6bf41349c3"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "pqid": [
                    "2581864003"
                ],
                "score": [
                    "0.06546396"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 18:06:31&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-proquest_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=Levels+of+explainable+artificial+intelligence+for+human-aligned+conversational+explanations&rft.jtitle=Artificial+intelligence&rft.au=Dazeley%2C+Richard&rft.date=2021-10&rft.volume=299&rft.spage=103525&rft.pages=103525-&rft.artnum=103525&rft.issn=0004-3702&rft.eissn=1872-7921&rft_id=info:doi/10.1016%2Fj.artint.2021.103525&rft.pub=Elsevier+B.V&rft.place=Amsterdam&rft_dat=<proquest_cross>2581864003</proquest_cross>&svc_dat=viewit&rft_pqid=2581864003"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-c380t-363c64bcb035a4c22b48e20bd103d7ae9bc1ea7a93bf4f6c8667c4e6bf41349c3"
                ],
                "citedby": [
                    "FETCH-LOGICAL-c380t-363c64bcb035a4c22b48e20bd103d7ae9bc1ea7a93bf4f6c8667c4e6bf41349c3"
                ]
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_proquest_journals_2581864003"
    },
    {
        "pnx": {
            "links": {
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "This survey emphasizes the significance of Explainable AI (XAI) techniques in detecting hateful speech and misinformation/Fake news. It explores recent trends in detecting these phenomena, highlighting current research that reveals a synergistic relationship between them. Additionally, it presents recent trends in the use of XAI methods to mitigate the occurrences of hateful land Fake contents in conversations. The survey reviews state-of-the-art XAI approaches, algorithms, modeling datasets, as well as the evaluation metrics leveraged for assessing model interpretability, and thus provides a comprehensive summary table of the literature surveyed and relevant datasets. It concludes with an overview of key observations, offering insights into the prominent model explainability methods used in hate speech and misinformation detection. The research strengths, limitations are also presented, as well as perspectives and suggestions for future directions in this research domain."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-8524-3559",
                    "https://orcid.org/0000-0003-3638-3464",
                    "https://orcid.org/0000-0002-9020-3885",
                    "https://orcid.org/0000-0002-9696-2372",
                    "https://orcid.org/0000-0001-9815-9295"
                ],
                "creator": [
                    "Ngueajio, Mikel",
                    "Aryal, Saurav",
                    "Atemkeng, Marcellin",
                    "Washington, Gloria",
                    "Rawat, Danda"
                ],
                "title": [
                    "Decoding Fake News and Hate Speech: A Survey of Explainable AI Techniques",
                    "ACM computing surveys"
                ],
                "subject": [
                    "High performance computing",
                    "Information storage and retrieval systems"
                ],
                "issn": [
                    "0360-0300",
                    "1557-7341"
                ],
                "fulltext": [
                    "true"
                ],
                "addtitle": [
                    "ACM CSUR"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20250731"
                ],
                "enddate": [
                    "20250731"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNo90DFPwzAQBWALgUQoiJ3JG1PgLmc7FVtUWlqpgqHZIzu-QCBNQkyB_nuKWpje8D694QlxiXCDqPQtpYiY0JGIUOs0TknhsYiADMRAAKfiLIRXAEgUmkgs7rnsfN0-y5l9Y_nIX0Ha1su5_WC56pnLlzuZydVm-OSt7Co5_e4bW7fWNSyzhcx3oK3fNxzOxUllm8AXhxyJfDbNJ_N4-fSwmGTL2Gqk2HuybPUYjUpBU2JYO0BDpIwHJpVg4sbaV8op7RRXzuEYEnQpVaUB8jQS1_vZcuhCGLgq-qFe22FbIBS_BxSHA3byai9tuf5Hf-UPDSJSmQ"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2025"
                ],
                "creatorcontrib": [
                    "Ngueajio, Mikel",
                    "Aryal, Saurav",
                    "Atemkeng, Marcellin",
                    "Washington, Gloria",
                    "Rawat, Danda"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "This survey emphasizes the significance of Explainable AI (XAI) techniques in detecting hateful speech and misinformation/Fake news. It explores recent trends in detecting these phenomena, highlighting current research that reveals a synergistic relationship between them. Additionally, it presents recent trends in the use of XAI methods to mitigate the occurrences of hateful land Fake contents in conversations. The survey reviews state-of-the-art XAI approaches, algorithms, modeling datasets, as well as the evaluation metrics leveraged for assessing model interpretability, and thus provides a comprehensive summary table of the literature surveyed and relevant datasets. It concludes with an overview of key observations, offering insights into the prominent model explainability methods used in hate speech and misinformation detection. The research strengths, limitations are also presented, as well as perspectives and suggestions for future directions in this research domain."
                ],
                "publisher": [
                    "New York, NY: ACM"
                ],
                "ispartof": [
                    "ACM computing surveys, 2025-07, Vol.57 (7), p.1-37"
                ],
                "identifier": [
                    "ISSN: 0360-0300",
                    "EISSN: 1557-7341",
                    "DOI: 10.1145/3711123"
                ],
                "rights": [
                    "Copyright held by the owner/author(s)."
                ],
                "snippet": [
                    "This survey emphasizes the significance of Explainable AI (XAI) techniques in detecting hateful speech and misinformation/Fake news..."
                ],
                "creator": [
                    "Ngueajio, Mikel ; Aryal, Saurav ; Atemkeng, Marcellin ; Washington, Gloria ; Rawat, Danda"
                ],
                "title": [
                    "Decoding Fake News and Hate Speech: A Survey of Explainable AI Techniques"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "subject": [
                    "High performance computing ;  Information storage and retrieval systems"
                ],
                "keyword": [
                    "Applied computing ;  Applied computing / Computers in other domains ;  Applied computing / Computers in other domains / Computing in government ;  Applied computing / Computers in other domains / Computing in government / Voting / election technologies ;  Computing methodologies / Artificial intelligence ;  Computing methodologies / Artificial intelligence / Natural language processing ;  Computing methodologies / Artificial intelligence / Natural language processing / Discourse, dialogue and pragmatics ;  Computing methodologies / Machine learning ;  Computing methodologies / Machine learning / Learning paradigms ;  Computing methodologies / Machine learning / Learning paradigms / Reinforcement learning ;  Computing methodologies / Machine learning / Learning paradigms / Reinforcement learning / Adversarial learning ;  Human-centered computing ;  Human-centered computing / Collaborative and social computing ;  Human-centered computing / Collaborative and social computing / Empirical studies in collaborative and social computing ;  Human-centered computing / Human computer interaction (HCI) ;  Human-centered computing / Human computer interaction (HCI) / HCI design and evaluation methods ;  Human-centered computing / Human computer interaction (HCI) / HCI design and evaluation methods / User studies ;  Information systems / Information systems applications ;  Information systems / Information systems applications / Collaborative and social computing systems and tools ;  Information systems / Information systems applications / Collaborative and social computing systems and tools / Social networking sites ;  Information systems / World Wide Web ;  Information systems / World Wide Web / Web applications ;  Information systems / World Wide Web / Web applications / Social networks"
                ]
            },
            "facets": {
                "creationdate": [
                    "2025"
                ],
                "creatorcontrib": [
                    "Ngueajio, Mikel",
                    "Aryal, Saurav",
                    "Atemkeng, Marcellin",
                    "Washington, Gloria",
                    "Rawat, Danda"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a513-dd3aea58164705326e5b0163346d0e34212b85df4b45b4efbb18021b73fc603d3"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "ACM computing surveys"
                ],
                "topic": [
                    "High performance computing",
                    "Information storage and retrieval systems"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "cop": [
                    "New York, NY"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3711123"
                ],
                "tpages": [
                    "37"
                ],
                "au": [
                    "Ngueajio, Mikel",
                    "Aryal, Saurav",
                    "Atemkeng, Marcellin",
                    "Washington, Gloria",
                    "Rawat, Danda"
                ],
                "atitle": [
                    "Decoding Fake News and Hate Speech: A Survey of Explainable AI Techniques"
                ],
                "stitle": [
                    "ACM CSUR"
                ],
                "date": [
                    "2025-07-31"
                ],
                "risdate": [
                    "2025"
                ],
                "volume": [
                    "57"
                ],
                "issue": [
                    "7"
                ],
                "spage": [
                    "1"
                ],
                "epage": [
                    "37"
                ],
                "pages": [
                    "1-37"
                ],
                "eissn": [
                    "1557-7341"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0002-8524-3559",
                    "https://orcid.org/0000-0003-3638-3464",
                    "https://orcid.org/0000-0002-9020-3885",
                    "https://orcid.org/0000-0002-9696-2372",
                    "https://orcid.org/0000-0001-9815-9295"
                ],
                "issn": [
                    "0360-0300"
                ],
                "abstract": [
                    "This survey emphasizes the significance of Explainable AI (XAI) techniques in detecting hateful speech and misinformation/Fake news. It explores recent trends in detecting these phenomena, highlighting current research that reveals a synergistic relationship between them. Additionally, it presents recent trends in the use of XAI methods to mitigate the occurrences of hateful land Fake contents in conversations. The survey reviews state-of-the-art XAI approaches, algorithms, modeling datasets, as well as the evaluation metrics leveraged for assessing model interpretability, and thus provides a comprehensive summary table of the literature surveyed and relevant datasets. It concludes with an overview of key observations, offering insights into the prominent model explainability methods used in hate speech and misinformation detection. The research strengths, limitations are also presented, as well as perspectives and suggestions for future directions in this research domain."
                ],
                "jtitle": [
                    "ACM computing surveys"
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Decoding Fake News and Hate Speech: A Survey of Explainable AI Techniques"
                ],
                "creationdate": [
                    "20250731"
                ],
                "author": [
                    "Ngueajio, Mikel ; Aryal, Saurav ; Atemkeng, Marcellin ; Washington, Gloria ; Rawat, Danda"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "acm_cross"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_1145_3711123"
                ],
                "addsrcrecordid": [
                    "eNo90DFPwzAQBWALgUQoiJ3JG1PgLmc7FVtUWlqpgqHZIzu-QCBNQkyB_nuKWpje8D694QlxiXCDqPQtpYiY0JGIUOs0TknhsYiADMRAAKfiLIRXAEgUmkgs7rnsfN0-y5l9Y_nIX0Ha1su5_WC56pnLlzuZydVm-OSt7Co5_e4bW7fWNSyzhcx3oK3fNxzOxUllm8AXhxyJfDbNJ_N4-fSwmGTL2Gqk2HuybPUYjUpBU2JYO0BDpIwHJpVg4sbaV8op7RRXzuEYEnQpVaUB8jQS1_vZcuhCGLgq-qFe22FbIBS_BxSHA3byai9tuf5Hf-UPDSJSmQ"
                ],
                "sourcerecordid": [
                    "3711123"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a513-dd3aea58164705326e5b0163346d0e34212b85df4b45b4efbb18021b73fc603d3"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.06377552"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 18:06:31&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=Decoding+Fake+News+and+Hate+Speech%3A+A+Survey+of+Explainable+AI+Techniques&rft.jtitle=ACM+computing+surveys&rft.au=Ngueajio%2C+Mikel&rft.date=2025-07-31&rft.volume=57&rft.issue=7&rft.spage=1&rft.epage=37&rft.pages=1-37&rft.issn=0360-0300&rft.eissn=1557-7341&rft_id=info:doi/10.1145%2F3711123&rft.pub=ACM&rft.place=New+York%2C+NY&rft.stitle=ACM+CSUR&rft_dat=<acm_cross>3711123</acm_cross>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-a513-dd3aea58164705326e5b0163346d0e34212b85df4b45b4efbb18021b73fc603d3"
                ],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_1145_3711123"
    },
    {
        "pnx": {
            "links": {
                "linktopdf": [
                    "$$Uhttps://dl.acm.org/doi/pdf/10.1145/3665647$$EPDF$$P50$$Gacm$$Hfree_for_read"
                ],
                "openurlfulltext": [
                    "$$Topenurlfull_article"
                ],
                "openurl": [
                    "$$Topenurl_article"
                ],
                "thumbnail": [
                    "$$Tsyndetics_thumb_exl"
                ]
            },
            "search": {
                "description": [
                    "Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with \\(N=501\\) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully."
                ],
                "orcidid": [
                    "https://orcid.org/0000-0001-9186-2092",
                    "https://orcid.org/0000-0002-9062-0996",
                    "https://orcid.org/0000-0003-3447-0556",
                    "https://orcid.org/0000-0002-0249-4062",
                    "https://orcid.org/0000-0003-4101-5180"
                ],
                "creator": [
                    "Humer, Christina",
                    "Hinterreiter, Andreas",
                    "Leichtmann, Benedikt",
                    "Mara, Martina",
                    "Streit, Marc"
                ],
                "title": [
                    "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions",
                    "ACM transactions on interactive intelligent systems"
                ],
                "subject": [
                    "Artificial intelligence",
                    "High performance computing"
                ],
                "issn": [
                    "2160-6455",
                    "2160-6463"
                ],
                "fulltext": [
                    "true"
                ],
                "addtitle": [
                    "ACM TIIS"
                ],
                "general": [
                    "ACM"
                ],
                "startdate": [
                    "20240803"
                ],
                "enddate": [
                    "20240803"
                ],
                "recordtype": [
                    "article"
                ],
                "recordid": [
                    "eNo9kEFPwzAMhSMEEtOYuHPKjQuFZGmShdtUNjZpExICxK3KEhsKazs17YF_T8rGLFl-tj_78Ai55OyW81TeCaWkSvUJGYy5YolKlTg9ainPySiELxZDSiGFHpC3Z7AhdE1RfdzQdRG2YP2ffoBNV31HeU-zutzZnqAzRHBtoDXS9-mSrqH9rH1sK7roSlvFI1eEoq7CBTlDuw0wOtQheZ3PXrJFsnp6XGbTVWLHwrSJR8dRcMEZeG0YuInrcyO0kEbFETIJTBoQSqO3KaDBFKRHbbT2EyWG5Hr_1zV1CA1gvmuK0jY_OWd570h-cCSSV3vSuvII_S9_AdcfWrA"
                ],
                "scope": [
                    "AAYXX",
                    "CITATION"
                ],
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Humer, Christina",
                    "Hinterreiter, Andreas",
                    "Leichtmann, Benedikt",
                    "Mara, Martina",
                    "Streit, Marc"
                ],
                "rsrctype": [
                    "article"
                ]
            },
            "delivery": {
                "fulltext": [
                    "fulltext"
                ],
                "delcategory": [
                    "Remote Search Resource"
                ]
            },
            "display": {
                "description": [
                    "Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with \\(N=501\\) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully."
                ],
                "publisher": [
                    "New York, NY: ACM"
                ],
                "ispartof": [
                    "ACM transactions on interactive intelligent systems, 2024-08, Vol.14 (3), p.1-36, Article 16"
                ],
                "identifier": [
                    "ISSN: 2160-6455",
                    "EISSN: 2160-6463",
                    "DOI: 10.1145/3665647"
                ],
                "rights": [
                    "Copyright held by the owner/author(s)."
                ],
                "snippet": [
                    "...), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI..."
                ],
                "creator": [
                    "Humer, Christina ; Hinterreiter, Andreas ; Leichtmann, Benedikt ; Mara, Martina ; Streit, Marc"
                ],
                "title": [
                    "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions"
                ],
                "lds50": [
                    "peer_reviewed"
                ],
                "type": [
                    "article"
                ],
                "source": [
                    "ACM Digital Library Complete"
                ],
                "language": [
                    "eng"
                ],
                "oa": [
                    "free_for_read"
                ],
                "subject": [
                    "Artificial intelligence ;  High performance computing"
                ],
                "keyword": [
                    "Empirical studies in HCI ;  Human-centered computing"
                ]
            },
            "facets": {
                "creationdate": [
                    "2024"
                ],
                "creatorcontrib": [
                    "Humer, Christina",
                    "Hinterreiter, Andreas",
                    "Leichtmann, Benedikt",
                    "Mara, Martina",
                    "Streit, Marc"
                ],
                "language": [
                    "eng"
                ],
                "toplevel": [
                    "peer_reviewed",
                    "online_resources"
                ],
                "frbrgroupid": [
                    "cdi_FETCH-LOGICAL-a239t-dfc1f31310ed790ec8cec8cb373596d79f05e059e367fda4ef9f4e5df7977d863"
                ],
                "frbrtype": [
                    "5"
                ],
                "jtitle": [
                    "ACM transactions on interactive intelligent systems"
                ],
                "topic": [
                    "Artificial intelligence",
                    "High performance computing"
                ],
                "prefilter": [
                    "articles"
                ],
                "rsrctype": [
                    "articles"
                ],
                "collection": [
                    "CrossRef"
                ]
            },
            "addata": {
                "format": [
                    "journal"
                ],
                "ristype": [
                    "JOUR"
                ],
                "cop": [
                    "New York, NY"
                ],
                "pub": [
                    "ACM"
                ],
                "doi": [
                    "10.1145/3665647"
                ],
                "tpages": [
                    "36"
                ],
                "au": [
                    "Humer, Christina",
                    "Hinterreiter, Andreas",
                    "Leichtmann, Benedikt",
                    "Mara, Martina",
                    "Streit, Marc"
                ],
                "atitle": [
                    "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions"
                ],
                "stitle": [
                    "ACM TIIS"
                ],
                "date": [
                    "2024-08-03"
                ],
                "risdate": [
                    "2024"
                ],
                "volume": [
                    "14"
                ],
                "issue": [
                    "3"
                ],
                "spage": [
                    "1"
                ],
                "epage": [
                    "36"
                ],
                "pages": [
                    "1-36"
                ],
                "artnum": [
                    "16"
                ],
                "eissn": [
                    "2160-6463"
                ],
                "orcidid": [
                    "https://orcid.org/0000-0001-9186-2092",
                    "https://orcid.org/0000-0002-9062-0996",
                    "https://orcid.org/0000-0003-3447-0556",
                    "https://orcid.org/0000-0002-0249-4062",
                    "https://orcid.org/0000-0003-4101-5180"
                ],
                "issn": [
                    "2160-6455"
                ],
                "abstract": [
                    "Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with \\(N=501\\) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully."
                ],
                "jtitle": [
                    "ACM transactions on interactive intelligent systems"
                ],
                "genre": [
                    "article"
                ],
                "oa": [
                    "free_for_read"
                ]
            },
            "sort": {
                "title": [
                    "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions"
                ],
                "creationdate": [
                    "20240803"
                ],
                "author": [
                    "Humer, Christina ; Hinterreiter, Andreas ; Leichtmann, Benedikt ; Mara, Martina ; Streit, Marc"
                ]
            },
            "control": {
                "iscdi": [
                    "true"
                ],
                "recordtype": [
                    "article"
                ],
                "sourceid": [
                    "acm_cross"
                ],
                "recordid": [
                    "cdi_crossref_primary_10_1145_3665647"
                ],
                "addsrcrecordid": [
                    "eNo9kEFPwzAMhSMEEtOYuHPKjQuFZGmShdtUNjZpExICxK3KEhsKazs17YF_T8rGLFl-tj_78Ai55OyW81TeCaWkSvUJGYy5YolKlTg9ainPySiELxZDSiGFHpC3Z7AhdE1RfdzQdRG2YP2ffoBNV31HeU-zutzZnqAzRHBtoDXS9-mSrqH9rH1sK7roSlvFI1eEoq7CBTlDuw0wOtQheZ3PXrJFsnp6XGbTVWLHwrSJR8dRcMEZeG0YuInrcyO0kEbFETIJTBoQSqO3KaDBFKRHbbT2EyWG5Hr_1zV1CA1gvmuK0jY_OWd570h-cCSSV3vSuvII_S9_AdcfWrA"
                ],
                "sourcerecordid": [
                    "3665647"
                ],
                "originalsourceid": [
                    "FETCH-LOGICAL-a239t-dfc1f31310ed790ec8cec8cb373596d79f05e059e367fda4ef9f4e5df7977d863"
                ],
                "sourcetype": [
                    "Aggregation Database"
                ],
                "sourceformat": [
                    "XML"
                ],
                "sourcesystem": [
                    "Other"
                ],
                "score": [
                    "0.0626036"
                ]
            }
        },
        "delivery": {
            "link": [],
            "deliveryCategory": [
                "Remote Search Resource"
            ],
            "availability": [
                "fulltext"
            ],
            "displayLocation": false,
            "additionalLocations": false,
            "physicalItemTextCodes": "",
            "feDisplayOtherLocations": false,
            "displayedAvailability": "true",
            "holding": [],
            "almaOpenurl": "https://na03.alma.exlibrisgroup.com/view/uresolver/01CASLS_REGINA/openurl?ctx_enc=info:ofi/enc:UTF-8&ctx_id=10_1&ctx_tim=2025-03-06 18:06:31&ctx_ver=Z39.88-2004&url_ctx_fmt=info:ofi/fmt:kev:mtx:ctx&url_ver=Z39.88-2004&rfr_id=info:sid/primo.exlibrisgroup.com-acm_cross&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.genre=article&rft.atitle=Reassuring%2C+Misleading%2C+Debunking%3A+Comparing+Effects+of+XAI+Methods+on+Human+Decisions&rft.jtitle=ACM+transactions+on+interactive+intelligent+systems&rft.au=Humer%2C+Christina&rft.date=2024-08-03&rft.volume=14&rft.issue=3&rft.spage=1&rft.epage=36&rft.pages=1-36&rft.artnum=16&rft.issn=2160-6455&rft.eissn=2160-6463&rft_id=info:doi/10.1145%2F3665647&rft.pub=ACM&rft.place=New+York%2C+NY&rft.stitle=ACM+TIIS&rft_dat=<acm_cross>3665647</acm_cross>&svc_dat=viewit"
        },
        "context": "PC",
        "adaptor": "Primo Central",
        "extras": {
            "citationTrails": {
                "citing": [
                    "FETCH-LOGICAL-a239t-dfc1f31310ed790ec8cec8cb373596d79f05e059e367fda4ef9f4e5df7977d863"
                ],
                "citedby": []
            },
            "timesCited": {}
        },
        "@id": "https://na03.alma.exlibrisgroup.com/primaws/rest/pub/pnxs/PC/cdi_crossref_primary_10_1145_3665647"
    }

]


module.exports = {
    HCIData
};